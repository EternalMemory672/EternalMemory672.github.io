# 图神经网络代码分析

## TGN（Temporal Graph Networks）

[twitter-research/tgn: TGN: Temporal Graph Networks (github.com)](https://github.com/twitter-research/tgn)

### Dataset format

The networks are stored under the `data/` folder, one file per network. The filename should be `<network>.csv`.

The network should be in the following format:

-   One line per interaction/edge.
-   Each line should be: *user, item, timestamp, state label, comma-separated array of features*.
-   First line is the network format.
-   *User* and *item* fields can be alphanumeric.
-   *Timestamp* should be in cardinal format (not in datetime).
-   *State label* should be 1 whenever the user state changes, 0 otherwise. If there are no state labels, use 0 for all interactions.
-   *Feature list* can be as long as desired. It should be atleast 1 dimensional. If there are no features, use 0 for all interactions.

For example, the first few lines of a dataset can be:

```
user,item,timestamp,state_label,comma_separated_list_of_features
0,0,0.0,0,0.1,0.3,10.7
2,1,6.0,0,0.2,0.4,0.6
5,0,41.0,0,0.1,15.0,0.6
3,2,49.0,1,100.7,0.8,0.9
```



### preprocess_data

**分离数据集为基础信息、边特征、节点特征，差异化源和目的准备创建二分图，存储三个矩阵**

#### functions

main() --data wikipedia --bipartite

preprocess(data_name)

reindex(df,bipartite=True)

run(data_name,bipartite=True)



#### workflow

main -> run -> preprocess -> reindex (-> save)



#### [in]

python ./utils/preprocess_data.py --data wikipedia --bipartite

./utils/data/wikipedia.csv

| user_id | item_id | timestamp | state_label | comma_separated_list_of_features |      |
| ------- | ------- | --------- | ----------- | -------------------------------- | ---- |
| 0       | 0       | 0.0       | 0           | -0.17506251172115117             | ...  |



#### main() - 主函数

args.data = "wikipedia"

args.bipartite = True

run(args.data,bipartite=args.bipartite)

#### run() - 执行函数

- PATH ./utils/data/ml_wikipedia.csv 

基础信息：源、目、时间、标签、索引

| \<anonymous\> | u    | i    | ts    | label | idx  |
| ------------- | ---- | ---- | ----- | ----- | ---- |
| 0             | 1    | 8228 | 0.0   | 0.0   | 1    |
| 1             | 2    | 8229 | 36.0  | 0.0   | 2    |
| 2             | 2    | 8229 | 77.0  | 0.0   | 3    |
| 3             | 3    | 8230 | 131.0 | 0.0   | 4    |

\<anonymous\> - 数组下标？

u - 源 **源IP地址**

i - 目的 **目的IP地址**

ts - 时间戳 **时间戳**

label - 标签（异常） **标签**

idx - 索引 **索引**

- OUT_FEAT ./utils/data/ml_wikipedia.npy

边属性矩阵？

- OUT_NODE_FEAT ./utils/data/ml_wikipedia_node.npy

节点属性矩阵？

- df, feat = preprocess("./utils/data/wikipedia.csv")
- new_df = reindex(df,bipartite)
- 创造feat的空列
- 合并空列和feat
- u和i间的取最大索引
- 创造最大索引+1行==172（comma_separated_list_of_features的列数）==列的零矩阵**为了匹配边特征矩阵？**



#### preprocess() - 预处理函数

取出抬头，以e[0]（user_id）为源（u），追加到u_list列表中。

user_id -> u -> u_list

item_id -> i -> i_list

timestamp -> ts -> ts_list

State_label -> label -> label_list

comma_separated_list_of_features -> feat(np.array) -> feat_l

返回pd.DataFrame，为上述前四个列表作为列的数据表和属性组成的np数组



#### reindex() - 索引重构函数

bipartite 二分图、二部图

创建df的拷贝

1. 如果需要二分图

​	判断u、i两列去重后的长度是否为最大值-最小值+1即是否每个最大值与最小值之间的数均有对应的边

​	令所有目的均加上源的最大值使图二分

​	u、i、idx自增除零

2. 无需二分图

​	u、i、idx自增除零

返回新的DataFrame



#### [out]

PATH ./utils/data/ml_wikipedia.csv 

OUT_FEAT ./utils/data/ml_wikipedia.npy

OUT_NODE_FEAT ./utils/data/ml_wikipedia_node.npy



### train_self_supervised

#### functions



#### workflow

MKDIR -> SET_UP_LOGGER -> get_data() -> get_neighbor_finder() -> RandEdgeSampler() 

#### [in]

python train_self_supervised.py --use_memory --prefix tgn-attn --n_runs 10 --n_epoch 20

#### MKDIR

```python
MODEL_SAVE_PATH = f'./saved_models/{args.prefix}-{args.data}.pth'
get_checkpoint_path = lambda epoch: f'./saved_checkpoints/{args.prefix}-{args.data}-{epoch}.pth'
```



#### SET_UP_LOGGER



#### get_data()

-   读入数据预处理部分产生的三个文件
-   生成0.75和0.85分位处的时间（ts）**作为划分训练和测试集的标准？**
-   读出source(u)、destination(i)、edge_idx(idx)、labels(label)、timestamp(ts)五个字段的数据，构造Data类full_data
-   设置随机数种子2020
-   node_set节点集为sources去重和destinations去重后的并集，n_total_unique_nodes为其长度
-   取测试节点集合为源目节点中时间戳大于val_time的节点
-   随机取10%测试节点并随机排列作为新测试节点集（我们保留作为新节点的样本节点以测试归纳性，因此我们必须从训练集中移除它们的所有边）
-   做源目节点集的除测试节点集的掩码集

![image-20221025221658123](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025221658123.png)

-   只有两个掩码集都为假时才能取边，做训练掩码集要求时间戳小于等于val_time且满足两个掩码集都为假

![image-20221025222134420](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025222134420.png)

-   **做训练集**，做训练节点集
-   做新增节点集为所有节点去掉训练节点

>   `val`是`validation`的简称。
>   `training dataset`和`validation dataset`都是在训练的时候起作用。
>   而因为`validation`的数据集和`training`没有交集，所以这部分数据对最终训练出的模型没有贡献。
>   `validation`的主要作用是来验证是否过拟合、以及用来调节训练参数等。
>
>   比如训练`0-10000`次迭代过程中，`train`和`validation`的`loss`都是不断降低，
>   但是从`10000-20000`过程中`train loss`不断降低，`validation`的`loss`不降反升。
>   那么就证明继续训练下去，模型只是对`training dataset`这部分拟合的特别好，但是泛化能力很差。
>   所以与其选取`20000`次的结果，不如选择`10000`次的结果。
>   这个过程的名字叫做`Early Stop`，`validation`数据在此过程中必不可少。

-   做val和test的掩码集，做所有包含新节点边的掩码集合并拆分为val和test
-   **做val_data数据集**、**做test_data数据集**
-   做val_data数据集中包含新节点的数据集new_node_val_data、做test_data数据集中包含新节点的数据集new_node_test_data
-   输出必要信息

```
The dataset has 157474 interactions, involving 9227 different nodes
The training dataset has 81029 interactions, involving 6141 different nodes
The validation dataset has 23621 interactions, involving 3256 different nodes
The test dataset has 23621 interactions, involving 3564 different nodes
The new node validation dataset has 12016 interactions, involving 2120 different nodes
The new node test dataset has 11715 interactions, involving 2437 different nodes
922 nodes were used for the inductive testing, i.e. are never seen during training
```

-   返回node_features, edge_features, full_data, train_data, val_data, test_data,new_node_val_data, new_node_test_data

#### get_neighbor_finder()

初始化训练邻居查找器以检索时间图

-   传入集合，uniform=False
-   根据节点最大索引构造adj_list空列表
-   向adj_list中每个节点对应位置中追加邻居信息

![image-20221025225145212](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025225145212.png)

边1上的节点2与节点8228在时间0.0处相连

-   调用NeighborFinder函数
-   将neighbors按索引排序并把三个值分别组成的列表追加到类的对应列表中

![image-20221025225706216](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025225706216.png)

![image-20221025230029418](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025230029418.png)

-   同样的方法构造train和full的查找器

#### RandEdgeSampler()

初始化负采样器。为验证和测试设置种子，使不同运行中的负值相同。注意：在感应设置中，负值仅在其他新节点中采样

-   定义源目列表和种子（若有）

![image-20221025230546366](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025230546366.png)

#### SET_DEVICE

设置设备若GPU可用则用GPU否则CPU

#### compute_time_statistics()

-   传入源目列表和时间戳列表
-   依次取出边的三个要素并检查源目地址是否在字典中，不在则添加

![image-20221025231336920](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025231336920.png)

-   在对应的列表中追加时间偏移，更新源目节点的最后出现时间，如此遍历完传入列表

![image-20221025231612419](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025231612419.png)

-   计算源目时间偏移均值和标准差并返回四个值

![image-20221025231848801](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025231848801.png)

-   根据n_runs构造结果文件存储路径

![image-20221025232106121](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025232106121.png)

#### INITIALIZE_MODEL

根据提供的参数初始化模型

```
tgn = TGN(neighbor_finder=train_ngh_finder, node_features=node_features,
edge_features=edge_features, device=device,
n_layers=NUM_LAYER,
n_heads=NUM_HEADS, dropout=DROP_OUT, use_memory=USE_MEMORY,
message_dimension=MESSAGE_DIM, memory_dimension=MEMORY_DIM,
memory_update_at_start=not args.memory_update_at_end,
embedding_module_type=args.embedding_module,
message_function=args.message_function,
aggregator_type=args.aggregator,
memory_updater_type=args.memory_updater,
n_neighbors=NUM_NEIGHBORS,
mean_time_shift_src=mean_time_shift_src, std_time_shift_src=std_time_shift_src,
mean_time_shift_dst=mean_time_shift_dst, std_time_shift_dst=std_time_shift_dst,  use_destination_embedding_in_message=args.use_destination_embedding_in_message,        use_source_embedding_in_message=args.use_source_embedding_in_message,
dyrep=args.dyrep)
```

根据传入参数初始化类

![image-20221025232545759](2022-10-26-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/image-20221025232545759.png)



### 数据预预处理

仿造数据集wikipedia，接入到tgn中

```python
import pandas as pd
import ipaddress
import time


def addr2num(ip, port):
    bin_ip = bin(int(ipaddress.IPv4Address(ip))).replace("0b", "").zfill(32)
    # 将ip地址转化为32位2进制数
    bin_port = bin(port).replace('0b', '').zfill(16)
    # 将端口转化为16位2进制数
    id = bin_ip + bin_port
    id = int(id, 2)
    # print(id)
    return id


def main():
    df = pd.read_csv('./data/temp.csv')
    # 读入数据集
    df = df.drop(columns=["Flow ID", "Attack"])
    # 去掉流表识和攻击类型字段
    df.insert(loc=0, column='user_id', value=0)
    df["user_id"] = df.apply(lambda x: addr2num(x['Src IP'], int(x['Src Port'])), axis=1)
    # 添加user_id列为拼接源ip和端口的整数
    df.insert(loc=1, column='item_id', value=0)
    df["item_id"] = df.apply(lambda x: addr2num(x['Dst IP'], int(x['Dst Port'])), axis=1)
    # 添加item_id列为拼接目的ip和端口的整数
    df = df.drop(columns=['Src IP', 'Dst IP', 'Src Port', 'Dst Port'])
    # 去掉源目ip和端口字段
    temp_data = df.pop("Timestamp")
    df.insert(2, 'timestamp', temp_data)
    temp_data = df.pop("Label")
    df.insert(3, 'state_label', temp_data)
    # 重命名两列并调整位置
    df['timestamp'] = df['timestamp'].apply(lambda x: int(time.mktime(time.strptime(x, "%d/%m/%Y %I:%M:%S %p"))))
    # 将时间字符串转化为时间戳
    opt = list(df.columns.values)[4:]
    # 取需要归一化的字段名
    for name in opt:
        M = df[name].max()
        m = df[name].min()
        df[name] = df[name].apply(lambda x: ((x - m) / (M - m)) if (M - m) != 0 else 0)
    # max-min归一化
    src_set = df.user_id.values
    dst_set = df.item_id.values
    node_set = set(src_set).union(set(dst_set))  # 去重取并集构造节点集
    ordered_node_set = sorted(node_set)  # 递增排序
    assert (len(ordered_node_set) == len(set(ordered_node_set)))  # 查重
    df["user_id"] = df["user_id"].apply(lambda x: ordered_node_set.index(x) + 1)
    df["item_id"] = df["item_id"].apply(lambda x: ordered_node_set.index(x) + 1)
    # 将源目编码替换为对应的index以降低编码长度
    df.to_csv('./data/ml_temp.csv', index=False)
    # 输出
    print(df)


if __name__ == '__main__':
    main()
```

